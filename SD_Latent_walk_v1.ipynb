{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SD LATENT WALK WITH ARTISTIC MODELS\n",
        "# ==========================================\n",
        "\n",
        "import random\n",
        "import datetime\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler"
      ],
      "metadata": {
        "id": "bn_ai6sRkn_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 0: Choose Your Artistic Model\n",
        "# Uncomment the one you want to use:\n",
        "\n",
        "# Option 1: DreamShaper - Best all-around artistic model\n",
        "# MODEL_ID = \"Lykon/DreamShaper\"\n",
        "# MODEL_NAME = \"DreamShaper\"\n",
        "\n",
        "# Option 2: Deliberate - Photorealistic painting style\n",
        "# MODEL_ID = \"XpucT/Deliberate\"\n",
        "# MODEL_NAME = \"Deliberate\"\n",
        "\n",
        "# Option 3: Dreamlike Diffusion - Surreal, dreamy art\n",
        "MODEL_ID = \"dreamlike-art/dreamlike-diffusion-1.0\"\n",
        "# MODEL_NAME = \"Dreamlike\"\n",
        "\n",
        "# Option 4: ReV Animated - Fantasy/2.5D style\n",
        "# MODEL_ID = \"stablediffusionapi/rev-animated\"\n",
        "# MODEL_NAME = \"ReV-Animated\"\n",
        "\n",
        "# Option 5: OpenJourney - Midjourney-style art\n",
        "# MODEL_ID = \"prompthero/openjourney\"\n",
        "# MODEL_NAME = \"OpenJourney\"\n",
        "\n",
        "# print(f\"Loading {MODEL_NAME}...\")\n",
        "print(f\"Model ID: {MODEL_ID}\")"
      ],
      "metadata": {
        "id": "c6OkXXSYgb_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load model\n",
        "# if 'pipe' not in globals():\n",
        "print(\"Loading model...\")\n",
        "# model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "# model_id = \"Lykon/DreamShaper\"\n",
        "scheduler = DDIMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None,\n",
        "    requires_safety_checker=False\n",
        ").to(\"cuda\")\n",
        "# pipe.enable_vae_slicing()\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "# SLERP\n",
        "def slerp(val, low, high, DOT_THRESHOLD=0.9995):\n",
        "    \"\"\"\n",
        "    Spherical Linear Interpolation for latent tensors\n",
        "    low, high: tensors of shape (batch, channels, height, width)\n",
        "    \"\"\"\n",
        "    # Flatten spatial dimensions for dot product calculation\n",
        "    original_shape = low.shape\n",
        "    low_flat = low.reshape(original_shape[0], -1)\n",
        "    high_flat = high.reshape(original_shape[0], -1)\n",
        "\n",
        "    # Normalize\n",
        "    low_norm = low_flat / (torch.norm(low_flat, dim=1, keepdim=True) + 1e-8)\n",
        "    high_norm = high_flat / (torch.norm(high_flat, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "    # Calculate dot product (cosine of angle)\n",
        "    dot = (low_norm * high_norm).sum(1)\n",
        "\n",
        "    # If vectors are nearly parallel, use linear interpolation\n",
        "    # Use torch.where for tensor conditionals instead of Python if\n",
        "    dot_threshold = torch.tensor(DOT_THRESHOLD, device=dot.device)\n",
        "\n",
        "    # Calculate angle for slerp\n",
        "    theta = torch.acos(torch.clamp(dot, -1.0, 1.0))\n",
        "    sin_theta = torch.sin(theta)\n",
        "\n",
        "    # Interpolation weights\n",
        "    theta_0 = theta * (1 - val)\n",
        "    theta_1 = theta * val\n",
        "\n",
        "    # Compute slerp weights\n",
        "    w0 = torch.sin(theta_0) / (sin_theta + 1e-8)\n",
        "    w1 = torch.sin(theta_1) / (sin_theta + 1e-8)\n",
        "\n",
        "    # Reshape weights for broadcasting\n",
        "    w0 = w0.view(-1, 1, 1, 1)\n",
        "    w1 = w1.view(-1, 1, 1, 1)\n",
        "\n",
        "    # Interpolate\n",
        "    result = w0 * low + w1 * high\n",
        "\n",
        "    # Where dot > threshold, fall back to linear interpolation\n",
        "    mask = (dot.abs() > DOT_THRESHOLD).view(-1, 1, 1, 1)\n",
        "    linear_result = (1 - val) * low + val * high\n",
        "\n",
        "    return torch.where(mask, linear_result, result)\n",
        "\n",
        "# Simple linear interpolation (fallback)\n",
        "def lerp(val, low, high):\n",
        "    return low + (high - low) * val\n",
        "\n",
        "# ==========================================\n",
        "# MAIN VIDEO GENERATION FUNCTION\n",
        "# ==========================================\n",
        "\n",
        "def generate_latent_walk_video(\n",
        "    prompt=\"a futuristic cityscape at sunset, cyberpunk style, highly detailed, 8k\",\n",
        "    negative_prompt=\"blurry, low quality, distorted, ugly\",\n",
        "    num_frames=60,\n",
        "    fps=12,\n",
        "    height=512,\n",
        "    width=512,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=20,\n",
        "    walk_type=\"circular_noise\",\n",
        "    seed_start=42,\n",
        "    seed_end=12345,\n",
        "    output_path=\"latent_walk.mp4\",\n",
        "    reverse_loop=True\n",
        "):\n",
        "    device = pipe.device\n",
        "    latents_shape = (1, 4, height // 8, width // 8)\n",
        "\n",
        "    print(f\"\\nGenerating {num_frames} frames for: '{prompt}'\")\n",
        "    print(f\"Walk type: {walk_type}\")\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    # Encode text prompt\n",
        "    text_input = pipe.tokenizer(\n",
        "        [prompt],\n",
        "        padding=\"max_length\",\n",
        "        max_length=pipe.tokenizer.model_max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = text_input.input_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_embeddings = pipe.text_encoder(input_ids)[0]\n",
        "\n",
        "        uncond_input = pipe.tokenizer(\n",
        "            [negative_prompt] if negative_prompt else [\"\"],\n",
        "            padding=\"max_length\",\n",
        "            max_length=pipe.tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        uncond_ids = uncond_input.input_ids.to(device)\n",
        "        uncond_embeddings = pipe.text_encoder(uncond_ids)[0]\n",
        "\n",
        "        text_embeddings_base = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "    # Setup noise interpolation\n",
        "    if walk_type == \"circular_noise\":\n",
        "        torch.manual_seed(seed_start)\n",
        "        noise_x = torch.randn(latents_shape, device=device, dtype=torch.float16)\n",
        "        torch.manual_seed(seed_end)\n",
        "        noise_y = torch.randn(latents_shape, device=device, dtype=torch.float16)\n",
        "        angles = torch.linspace(0, 2 * np.pi, num_frames, device=device)\n",
        "\n",
        "    elif walk_type == \"linear_noise\":\n",
        "        generator_start = torch.Generator(device=device).manual_seed(seed_start)\n",
        "        generator_end = torch.Generator(device=device).manual_seed(seed_end)\n",
        "\n",
        "        noise_start = torch.randn(latents_shape, generator=generator_start, device=device, dtype=torch.float16)\n",
        "        noise_end = torch.randn(latents_shape, generator=generator_end, device=device, dtype=torch.float16)\n",
        "        interpolation_weights = torch.linspace(0, 1, num_frames, device=device)\n",
        "\n",
        "    # Generate frames\n",
        "    for i in tqdm(range(num_frames), desc=\"Generating frames\"):\n",
        "\n",
        "        if walk_type == \"circular_noise\":\n",
        "            current_noise = torch.cos(angles[i]) * noise_x + torch.sin(angles[i]) * noise_y\n",
        "        elif walk_type == \"linear_noise\":\n",
        "            t = interpolation_weights[i].item()  # Get scalar value\n",
        "            current_noise = slerp(t, noise_start, noise_end)\n",
        "\n",
        "        # Denoising loop\n",
        "        with torch.no_grad():\n",
        "            pipe.scheduler.set_timesteps(num_inference_steps)\n",
        "            latent_model_input = current_noise * pipe.scheduler.init_noise_sigma\n",
        "\n",
        "            for t in pipe.scheduler.timesteps:\n",
        "                latent_expanded = torch.cat([latent_model_input] * 2)\n",
        "                latent_expanded = pipe.scheduler.scale_model_input(latent_expanded, t)\n",
        "\n",
        "                noise_pred = pipe.unet(\n",
        "                    latent_expanded,\n",
        "                    t,\n",
        "                    encoder_hidden_states=text_embeddings_base\n",
        "                ).sample\n",
        "\n",
        "                noise_uncond, noise_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_uncond + guidance_scale * (noise_text - noise_uncond)\n",
        "                latent_model_input = pipe.scheduler.step(noise_pred, t, latent_model_input).prev_sample\n",
        "\n",
        "            # Decode to image\n",
        "            latent_model_input = 1 / 0.18215 * latent_model_input\n",
        "            image = pipe.vae.decode(latent_model_input).sample\n",
        "\n",
        "            image = (image / 2 + 0.5).clamp(0, 1)\n",
        "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
        "            image = (image * 255).astype(np.uint8)\n",
        "            frames.append(image)\n",
        "\n",
        "    # Create seamless loop\n",
        "    if reverse_loop and walk_type != \"circular_noise\":\n",
        "        print(\"Creating reverse loop...\")\n",
        "        frames = frames + frames[::-1]\n",
        "\n",
        "    # Save video\n",
        "    print(f\"\\nSaving video to {output_path}...\")\n",
        "    writer = imageio.get_writer(output_path, fps=fps, codec='libx264', quality=8)\n",
        "    for frame in frames:\n",
        "        writer.append_data(frame)\n",
        "    writer.close()\n",
        "\n",
        "    # Save GIF\n",
        "    # gif_path = output_path.replace('.mp4', '.gif')\n",
        "    # imageio.mimsave(gif_path, frames, fps=fps, loop=0)\n",
        "\n",
        "    print(f\"âœ“ Done! Duration: {len(frames)/fps:.1f}s, Frames: {len(frames)}\")\n",
        "    return frames, output_path\n",
        "\n",
        "# ==========================================\n",
        "# RUN EXAMPLES BELOW\n",
        "# ==========================================\n"
      ],
      "metadata": {
        "id": "s9VsF0Nz9uNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FRAME INTERPOLATION WITH RIFE\n",
        "Original file is located at https://colab.research.google.com/github/hzwer/ECCV2022-RIFE/blob/main/Colab_demo.ipynb\n",
        "\"\"\"\n",
        "\n",
        "!git clone https://github.com/hzwer/arXiv2020-RIFE\n",
        "!pip install git+https://github.com/rk-exxec/scikit-video.git@numpy_deprecation\n",
        "!mkdir /content/arXiv2020-RIFE/train_log\n",
        "%cd /content/arXiv2020-RIFE/train_log\n",
        "!gdown --id 1APIzVeI-4ZZCEuIRE1m6WYfSCaOsi_7_\n",
        "!7z e RIFE_trained_model_v3.6.zip\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "0XCO2yu9tKO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================\n",
        "# PROMPT\n",
        "#============================\n",
        "\n",
        "prompt=\"Silhouettes of angelic beings, totem animals, broad rough strokes , energy misty, breathtaking, oil painting style, artistic, aesthetic modern art, hyper-realism, trending on artstation\"\n",
        "outfile = \"_\".join(prompt.split()[:5])\n",
        "outfile = outfile.replace(\",\", \"\")\n",
        "current_time = datetime.datetime.now().strftime(\"%H%M\")\n",
        "outfile = current_time + \"_\" + outfile + \".mp4\"\n",
        "seed1 = random.randint(1, 99)\n",
        "seed2 = random.randint(8888, 9999)\n",
        "# outfile\n",
        "# seed1\n",
        "# seed2\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(outfile)\n",
        "print(seed1)\n",
        "print(seed2)"
      ],
      "metadata": {
        "id": "sA81MPgRmln1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "929f56bc"
      },
      "source": [
        "# PREVIEW\n",
        "# Generate a single frame using the existing prompt\n",
        "# We set num_frames=1 to get just one image\n",
        "#===================================================\n",
        "\n",
        "single_frame_data, _ = generate_latent_walk_video(\n",
        "    prompt=prompt,\n",
        "    walk_type=\"linear_noise\", # linear_noise or circular_noise, doesn't matter for 1 frame\n",
        "    seed_start=random.randint(1, 9999),\n",
        "    seed_end=random.randint(1, 9999),\n",
        "    num_frames=1,\n",
        "    fps=1, # Doesn't matter for a single image\n",
        "    width=960,\n",
        "    height=480,\n",
        "    num_inference_steps=15,\n",
        "    output_path=\"single_image.mp4\", # A dummy output path, as we only need the image data\n",
        "    reverse_loop=False # Not relevant for a single frame\n",
        ")\n",
        "\n",
        "# Display the generated image\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(single_frame_data[0])\n",
        "plt.axis('off')\n",
        "plt.title(outfile)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example : Linear Walk\n",
        "# ====================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"EXAMPLE 2: Linear Walk\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "frames2, video2 = generate_latent_walk_video(\n",
        "    prompt=prompt,\n",
        "    walk_type=\"linear_noise\",\n",
        "    seed_start=random.randint(1, 99),\n",
        "    seed_end=random.randint(8000, 9999),\n",
        "    num_frames=60,\n",
        "    fps=10,\n",
        "    width=960,\n",
        "    height=480,\n",
        "    num_inference_steps=15,\n",
        "    output_path=outfile,\n",
        "    reverse_loop=True\n",
        ")"
      ],
      "metadata": {
        "id": "Wq7uofNqFZtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RIFE 4X FRAMES\n",
        "# ======================================\n",
        "\n",
        "%cd /content/arXiv2020-RIFE\n",
        "vidfile = \"../\" + outfile\n",
        "!python3 inference_video.py --exp=2 --video={vidfile}\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "BltSZnvstidg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example : Circular Walk (Seamless Loop)\n",
        "# ========================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"EXAMPLE : Circular Walk\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "frames1, video1 = generate_latent_walk_video(\n",
        "    prompt=prompt,\n",
        "    walk_type=\"circular_noise\",\n",
        "    num_frames=20,\n",
        "    fps=10,\n",
        "    num_inference_steps=15,\n",
        "    width=960,\n",
        "    height=480,\n",
        "    output_path=outfile\n",
        ")\n"
      ],
      "metadata": {
        "id": "EIR0s8HbFVhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIND EXAMPLES OF SD PROMPTS ON THESE SITES"
      ],
      "metadata": {
        "id": "m3HxhR0UpHHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Site           | URL                                      | Best For                                  |\n",
        "| -------------- | ---------------------------------------- | ----------------------------------------- |\n",
        "| **PromptHero** | [prompthero.com](https://prompthero.com) | Curated artistic prompts with examples    |\n",
        "| **Lexica.art** | [lexica.art](https://lexica.art)         | Search 10M+ generated images with prompts |\n",
        "| **Civitai**    | [civitai.com](https://civitai.com)       | Model-specific prompts and workflows      |\n",
        "| **OpenArt**    | [openart.ai](https://openart.ai)         | Prompt search + prompt book library       |\n",
        "| **ArtHub.ai**  | [arthub.ai](https://arthub.ai)           | Community prompts with style tags         |\n"
      ],
      "metadata": {
        "id": "4427oYQMpXXv"
      }
    }
  ]
}